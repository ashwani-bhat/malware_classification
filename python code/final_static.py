
# coding: utf-8

# In[25]:


import os
import json
import re
import pandas as pd
import pickle
import sys

# In[26]:
   
if __name__ == 'main':
    print("inside main")
    path = sys.argv[1]
    # List of features
    listOfFeatures = ['Characteristics', 'MajorLinkerVersion', 'MinorLinkerVersion', 'SizeOfCode', 'SizeOfInitializedData', 'SizeOfUninitializedData', 'AddressOfEntryPoint', 'BaseOfCode', 'ImageBase', 'SectionAlignment', 'FileAlignment', 'MajorOperatingSystemVersion', 'MinorOperatingSystemVersion', 'MajorImageVersion', 'MinorImageVersion', 'MajorSubsystemVersion', 'MinorSubsystemVersion', 'SizeOfHeaders', 'CheckSum', 'Subsystem', 'DllCharacteristics', 'SizeOfStackReserve', 'SizeOfStackCommit', 'SizeOfHeapReserve', 'LoaderFlags']

    # Enter the path for the test file
    


    # In[27]:


    countLoop = 0
    mylist = list()
    apidict = dict()
    has = list()
    for filename in os.listdir(path):
        has.append(filename)
        mydict = dict()
        print('lfj')
        try:
            countLoop += 1
            f = open(path + '/' + filename+'/Structure_Info.txt',encoding = 'utf-8')
            struct = f.readlines()
            print('hle')
            f.close()
        except UnicodeDecodeError:
            continue
        
        raw = list()
        virtual = list()
        entropy = list()
        noOfSec =  0
        
        for lines in struct:
            # no of sections
            if lines == '[IMAGE_SECTION_HEADER]\n':
                noOfSec +=1
            
            line = lines.split()
                
            # for image section header
            if len(line) == 4:
                line[2] = line[2][:-1]
                if line[2] in listOfFeatures:
                    if line[2] not in mydict.keys():
                        mydict[line[2]] = int(line[3],16)
                elif line[2] == 'SizeOfRawData':  #for Raw data size
                    raw.append(int(line[3],16))
                elif line[2] == 'Misc_VirtualSize': #for virtual size
                    virtual.append(int(line[3],16))
                if line[0] == 'Entropy:':
                    entropy.append(float(line[1]))    
            
            # for dlls
            else:
                try:
                    apis = line[0].split('.')
                    if apis[1] == 'dll':
                        if apis[2] in apidict.keys():
                            apidict[apis[2]] = apidict[apis[2]] + 1
                        else:
                            apidict[apis[2]] = 1
                except:
                    continue
                    
        try:
            min_raw = min(raw)
            max_raw = max(raw)
            mean_raw = sum(raw) / len(raw) 
        except:
            min_raw = 0
            max_raw = 0
            mean_raw = 0
        try:
            min_virtual = min(virtual)
            max_virtual = min(virtual)
            mean_virtual = sum(virtual) / len(virtual)
        except:
            min_virtual = 0
            max_virtual = 0
            mean_virtual = 0
        try:
            min_ent = min(entropy)
            max_ent = max(entropy)
            mean_ent = sum(entropy) / len(entropy) 
        except:
            min_ent = 0
            max_ent = 0
            mean_ent = 0
        
        raw_dict = {'MinRawSize': min_raw, 'MaxRawSize': max_raw,'MeanRawSize': mean_raw}
        virtual_dict = {'MinVirtualSize': min_virtual,'MaxVirtualSize': max_virtual,'MeanVirtualSize': mean_virtual}
        entropy_dict = {'MinEntropy': min_ent, 'MaxEntropy': max_ent,'MeanEntropy': mean_ent}
        
        mydict.update(raw_dict)
        mydict.update(virtual_dict)
        mydict.update(entropy_dict)  
        mydict.update({'noOfSections':noOfSec})
        mylist.append(mydict)        


    # In[28]:


    p = ['MinRawSize','MaxRawSize','MeanRawSize','MinVirtualSize','MaxVirtualSize','MeanVirtualSize','MinEntropy','MaxEntropy','MeanEntropy']
    listOfFeatures.extend(p)
    listOfFeatures.extend(['noOfSections'])


    # In[29]:


    X_test = pd.DataFrame(mylist,columns = listOfFeatures) 


    # In[30]:


    X_test = X_test.fillna(0)
    X_test.head()


    # In[31]:


    for k in X_test.columns:
        if X_test[k].dtype != 'int64':
            X_test[k] = X_test[k].map(lambda name: int(name))


    # In[32]:


    # from sklearn import preprocessing
    # mm_scaler = preprocessing.MinMaxScaler()
    # X_test = mm_scaler.fit_transform(X_test)
    # X_test


    # In[33]:


    # load the model from disk

    filename = 'model.sav'
    loaded_model = pickle.load(open(filename, 'rb'))

    y_pred = loaded_model.predict(X_test)

    fam = {0 : 'B',1: 'M'}

    y_pred = [fam[x] for x in y_pred]

    import csv
    file = open('static.csv','w+')

    output = list()
    lis = ['hash_file', 'prediction label']
    writer = csv.writer(file, quoting=csv.QUOTE_NONNUMERIC)
    writer.writerow(lis)

    for hashes,pred in zip(has,y_pred):
    #     print(hashes,pred)
        lis = [hashes, pred]
        print(lis)
    #     print(lis)
        writer.writerow(lis)





